{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea575918",
   "metadata": {},
   "source": [
    "##Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d3fdf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #for mathematical computations\n",
    "import nltk        #NLP library \n",
    "import string      #process and handle strings in python\n",
    "import random      #randomize the replies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afae893",
   "metadata": {},
   "source": [
    "##How does a chatbot work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46439f3c",
   "metadata": {},
   "source": [
    "####Corpus:\n",
    "\n",
    "It is the training data needed for the chatbot to learn.\n",
    "Without a corpus, it is impossible for a chatbot to learn and reply something useful back to the user.\n",
    "\n",
    "####Data preprocessing - text case handling:\n",
    "\n",
    "Convert all the data coming as an input to either upper or lower case.\n",
    "Inaccurate data will lead the ML model to give inaccurate outputs.\n",
    "The data should be cleaned before the \n",
    "This will avoid misrepresentation and misinterpretation of words if spelt under lower or upper cases.\n",
    "\n",
    "####Tokenization\n",
    "\n",
    "Tokenization is the structured process of converting a sentence into individual collection of words. \n",
    "Break the sentences into individual collection of words.\n",
    "This will be used by the Chatbot to provide varied and specific answers to the questions asked.\n",
    "\n",
    "####Stemming \n",
    "\n",
    "Stemming is a process of finding similarities between words with the same root words. \n",
    "Instead of thinking about a million words. Finding the root word and then deciding on the words that stem from it is easier. \n",
    "Teach the chatbot to do this and turn the bot efficient and intelligent.\n",
    "This a powerful concept in etymology.\n",
    "\n",
    "####Generating a Bag of Words (BOW):\n",
    "\n",
    "It is the process of converting words into numbers by generating vector embeddings from the tokens generated.\n",
    "Each individual words are assigned of a number and randomized in the vector. \n",
    "This is now th ML algothrim can understand those words. \n",
    "\n",
    "####One hot encoding\n",
    "\n",
    "It is a process by which categorical variables are converted into a form that ML algorithms use.\n",
    "Although the order of works in not important, the word itself is important in this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58228d",
   "metadata": {},
   "source": [
    "##Importing and reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab04b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sumie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sumie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f=open('chatbot.txt','r',errors='ignore')\n",
    "raw_doc = f.read()\n",
    "\n",
    "#Data Preprocessing \n",
    "raw_doc = raw_doc.lower() \n",
    "\n",
    "#Using tokenizer punkt - it a pre trained tokenizer\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Converts doc to list of sentences\n",
    "sent_tokens = nltk.sent_tokenize(raw_doc)\n",
    "\n",
    "#Converts doc to list of words\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14aee6",
   "metadata": {},
   "source": [
    "##Verifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31846320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning is a branch of artificial intelligence (ai) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.',\n",
       " 'ibm has a rich history with machine learning.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1376249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine', 'learning']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595f745",
   "metadata": {},
   "source": [
    "##Text preprocessing\n",
    "\n",
    "Wordnet is a semantically-oriented dictionary of English included in NLTK. \n",
    "It has certain codes that knows how to deal with punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "279dff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return[lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e8c50",
   "metadata": {},
   "source": [
    "##Defining the greeting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4bd6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREET_INPUTS = (\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\", \"hey\", \"howdy\", \"doody\")\n",
    "GREET_RESPONSES = (\"hi\",\"hello\",\"*nods*\",\"hi there\",\"hello there\",\"howdydo\",\"I am glad! you are talking to me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51f2e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREET_INPUTS:\n",
    "            return random.choice(GREET_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd22c6",
   "metadata": {},
   "source": [
    "##Response generation \n",
    "\n",
    "Term frequency and inverse document frequency\n",
    "\n",
    "Term frequency - The number of times each word in your corpus is repeated. \n",
    "Inverse Document Frequency - Attach a component of rareness of the occurrance of the terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3411ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ffd5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo1_response = ''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        robo1_response = robo1_response + \"I am sorry! I don't understand you\"\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response+sent_tokens[idx]\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860c54b",
   "metadata": {},
   "source": [
    "##Defining conversation start/end protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f6b450e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: My name is Boby. Let's have a conversation! Also, if you want to exit any time, just type 'bye!'\n",
      "hello\n",
      "BOT: hello\n",
      "what is ML\n",
      "BOT:I am sorry! I don't understand you\n",
      "Machine Learning\n",
      "BOT:reinforcement machine learning\n",
      "reinforcement machine learning is a behavioral machine learning model that is similar to supervised learning, but the algorithm isnâ€™t trained using sample data.\n",
      "methods of machine learning\n",
      "BOT:machine learning methods\n",
      "machine learning classifiers fall into three primary categories.\n",
      "what are the three primary categories\n",
      "BOT:machine learning methods\n",
      "machine learning classifiers fall into three primary categories.\n",
      "ok\n",
      "BOT:I am sorry! I don't understand you\n",
      "ibm\n",
      "BOT:ibm has a rich history with machine learning.\n",
      "bye\n",
      "BOT: Bye! Take Care <3\n"
     ]
    }
   ],
   "source": [
    "flag = True \n",
    "print(\"BOT: My name is Boby. Let's have a conversation! Also, if you want to exit any time, just type 'bye!'\")\n",
    "while (flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response != 'bye'):\n",
    "        if(user_response == 'thanks' or user_response == 'thank you'):\n",
    "            flag=False\n",
    "            print(\"BOT : You are welcome. Toodle-o!\")\n",
    "        else: \n",
    "            if(greet(user_response)!=None):\n",
    "                print(\"BOT: \"+greet(user_response))\n",
    "            else:\n",
    "                sent_tokens.append(user_response)\n",
    "                word_tokens=word_tokens + nltk.word_tokenize(user_response)\n",
    "                final_words=list(set(word_tokens))\n",
    "                print(\"BOT:\",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"BOT: Bye! Take Care <3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b268797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
